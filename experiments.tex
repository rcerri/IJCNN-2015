\section{Experiments}\label{sec:experiments}

In this section, we present the experiments that were carried out to compare the prediction performance achieved by HMC-LMLP's versions and the state-of-the-art HMC~algorithms. We also present the datasets, parameters, and the evaluation measure that were employed in the experiments.

\subsection{Datasets}

We make use of ten freely available\footnote{\url{http://www.cs.kuleuven.be/~dtai/clus/hmcdatasets.html}} datasets related to protein function prediction. These datasets are related to issues like phenotype data and gene expression levels. Table~\ref{tab:datasets} presents the main characteristics of the training, validation, and test datasets. A description of each dataset can be found in \cite{Vens2008}.

Considering that there is no level definition in DAG structures (a class can be located at different levels depending on which hierarchical path is chosen from the root node to the class), we defined the depth of a class in a DAG structure as the deepest path from the class to the root node. This is necessary for the application of HMC-LMLP, since the method requires a clear separation of classes in levels. We chose the deepest path as the definition of depth because it guarantees that when a class is located in a level $l$, all its superclasses will be located in levels shallower~than~$l$. With this depth definition, each hierarchy ended up with 13~levels.

\begin{table*}[htpb]
\centering
\setlength{\tabcolsep}{6pt}
\caption{Summary of the datasets: number of attributes ($|A|$), number of classes ($|C|$), number of classes per level (Classes per level), total number of instances (Total) and number of multi-label instances~(Multi).}
\scriptsize
\begin{tabular*}{\textwidth}
    {@{\extracolsep{\fill}}lccccccccc}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{$|A|$}  & \multirow{2}{*}{$|C|$} & \multirow{2}{*}{Classes per level} & \multicolumn{ 2}{c}{Training} & \multicolumn{ 2}{c}{Valid} &  \multicolumn{ 2}{c}{Test} \\ 
 & & & & Total & Multi & Total & Multi & Total & Multi \\ \midrule
 
Cellcycle &  77  &  4122  & 33/155/394/597/929/779/631/335/171/63/21/5/9 & 1625  & 1625   &  848  &  848  &  1278  & 1278   \\

Church    &  27  &  4122  & 33/155/394/597/929/779/631/335/171/63/21/5/9 & 1627  &  1627  &  844  & 844  &  1278  & 1278 \\

Derisi    &  63  &  4116  & 33/155/394/596/927/778/630/334/171/63/21/5/9 & 1605  & 1605   &  842  &  842  &  1272  &  1272 \\

Eisen     &  79  &  3570  & 33/149/360/524/786/679/539/271/141/55/19/5/9 & 1055  &  1055  &  528  &  528  &  835  & 835 \\
  
Expr      &  551 &  4128  & 33/155/394/599/932/780/631/335/171/63/21/5/9 & 1636  &  1636  &  849  &  849  &  1288  & 1288 \\
  
Gasch1    &  173 &  4122  & 33/155/394/597/929/779/631/335/171/63/21/5/9 & 1631  &  1631  &  846  &  846  &  1281  & 1281 \\
  
Gasch2    &  52  &  4128  & 33/155/394/599/932/780/631/335/171/63/21/5/9 & 1636  &  1636  &  849  &  849  &  1288  & 1288 \\
  
Pheno     &  69  &  3124  & 33/145/332/489/670/568/460/236/114/49/18/4/6 & 653  &  653  &  352  &  352  &  581  & 581 \\
    
Seq       &  478 &  4130 & 33/155/394/599/932/780/633/335/171/63/21/5/9 & 1692  &  1692  &  876  &  876  &  1332   &  1332 \\
  
Spo       &  80  & 4116  & 33/155/394/596/927/778/630/334/171/63/21/5/9 & 1597  &  1597  & 837  &  837  &  1263  & 1263 \\
 
\bottomrule  
  
\end{tabular*}
\label{tab:datasets}
\end{table*}

We performed a pre-processing step before running HMC-LMLP over these datasets, in which all nominal attribute values were transformed into numeric values using the one-attribute-per-value approach. In this paper, instead of using $0$s and $1$s, the nominal attributes were assigned $-1$ (absence) and $1$ (presence), which are better suited for training neural networks \cite{Haykin1999}. The attributes were then standardized (mean $0$ and variance $1$). Additionally, all missing values for nominal and numeric attributes were replaced, respectively, by their mode and mean values.

\subsection{Evaluation Measure}

The outputs of HMC-LMLP, for each class, are real values between 0 and 1. The same is true for the literature methods. Thus, in order to obtain the final predictions, a threshold value was further employed. When classifying an instance, if the corresponding output value for a given class is greater than or equal to the threshold, the instance is assigned to the class, otherwise it is not.

The choice of the ``optimal" threshold value is a difficult task, since low threshold values lead to many classes being assigned to each instance, resulting in high recall and low precision. On the other hand, large threshold values lead to very few instances being classified, resulting in high precision and low recall. To deal with this problem, we make use of precision-recall curves (PR-curves) as the evaluation measure for the experiments. To obtain a PR-curve, different thresholds between [0,1] are applied to the outputs of the methods, and thus different values of precision and recall are obtained, one for each threshold value. Each threshold then represents a point within the PR-space. The union of these points form a PR-curve, and the area under the curve is calculated. All methods are thus compared based on their areas under~the~PR-curves.

More specifically, we employed the area under the average PR-curve ($AU(\overline{PRC})$). Given a threshold value, a precision-recall point ($\overline{Prec},\overline{Rec}$) in the PR-space can be obtained through Equations~(\ref{eq:prec})~and~(\ref{eq:reca}). They correspond to the micro-average of precision and recall.

\begin{multicols}{2}
	\begin{equation}
		\overline{Prec} = \frac{\sum_i TP_i}{\sum_i TP_i + \sum_i FP_i}
		\label{eq:prec}
	\end{equation}

	\begin{equation}
		\overline{Rec} = \frac{\sum_i TP_i}{\sum_i TP_i + \sum_i FN_i}
		\label{eq:reca}
	\end{equation}
\end{multicols}

To verify the significance of the results, we employed the Friedman and Nemenyi statistical tests, recommended for comparisons involving many classifiers and several datasets \cite{Demsar2006}. We adopted a confidence level of 95\% in the statistical~tests. As in \cite{Vens2008} and \cite{Otero2010}, 2/3 of each dataset were used for inducing the classification models and 1/3 for~test. We used the same data partitions suggested in \cite{Vens2008}.

\subsection{Parameters}

We investigate the performance of HMC-LMLP using the conventional Back-propagation algorithm (Bp) \cite{Rumelhart1986}, and the Resilient Back-propagation (Rprop)~\cite{Riedmiller1993}.

The HMC-LMLP parameters were optimized using the Eisen validation dataset. This dataset was selected because it was one of the datasets where Clus-HMC obtained its best performance (0.380), and also because it has a relatively small number of attributes, which allows to run several experiments in a reasonable amount of time. The following parameters were optimized: (i) number of neurons in each hidden layer (beginning with the hidden layer of the MLP network associated with the first hierarchical level, and finishing with the hidden layer of the MLP network associated with the last level), (ii) the learning rate and momentum constant used in the Back-propagation algorithm, and (iii) the range of values used to initialize the neural network's weights. %To reduce the influence of parameter selection in the MLP predictive performance, the number of hidden neurons was set as a fraction of the number of input~attributes.
We executed HMC-LMLP over the validation dataset using different sets of parameter values. We employed different initial weight values, number of hidden neurons, learning rates, and momentum constants. We did not use all possible sets of values due to the large number of possibilities.

\sloppy
For the initial weights, we noticed that the larger their values, the more likely was overfitting to occur (a better performance on more frequent classes but a worse overall prediction performance). We varied the initial weights by randomly selecting them initially from [-0.1,0.1], but gradually increasing the range to [-1,1]. We tested a limited number of neurons for each hidden layer, beginning with 1.0/1.0/0.95/0.9/0.85/0.8/0.75/0.7/0.65/0.6/0.55/0.5/0.45 neurons in each layer and gradually decreasing these values. These hidden neuron numbers represent the fraction of the total number of network inputs. Thus, if a neural network has 100 inputs, the value 0.6 means that it has actually 60 hidden neurons. Considering the learning rate and momentum, we started our experiments with the same default values used in the Weka toolkit~\cite{Weka}, in which the learning rate is set to 0.3 and the momentum to 0.2. We gradually decreased these values and noticed that the neural networks became less prone to overfitting as these values decreased. The final parameters obtained for HMC-LMLP after the preliminary experiments are listed~next.

\begin{itemize}
    \sloppy
	\item Number of hidden neurons per level (fraction of the total number of network inputs): 0.65/0.65/0.6/0.55/0.5/0.45/0.4/0.35/0.3/0.25/0.2/0.15/\\0.1;
	
	\item Learning rate and momentum constant used in Back-propagation for hidden and output layers: $\{0.05, 0.03\}$ and $\{0.03, 0.01\}$, respectively;

	\item Initial weights of the neural networks: within [-0.1,0.1];
	
	\item Parameter values of the Rprop algorithm: initial Delta ($\Delta_0$) = 0.1, maximum Delta ($\Delta_{max}$) = 50.0, minimum Delta ($\Delta_{min}$) = $1e^{-6}$, increase factor ($\eta^+$) = 1.2, and decrease factor ($\eta^-$) = 0.5. 
\end{itemize}

We would like to point out that we decreased the number of hidden neurons of the neural networks as the hierarchical levels became deeper. Our intention was to avoid overfitting, since the number of training instances is smaller for the networks associated with deeper hierarchical levels. The first two values (0.65/0.65) are the same because all instances are classified in classes belonging to the first and second levels. The Rprop parameter values were the ones suggested in \cite{Riedmiller1993}.

\subsection{Results}

Table~\ref{tab:prcurves} presents the PR-curves for the HMC-LMLP's versions and the literature methods. We refer to the HMC-LMLP versions as Bp-Comp (Back-propagation with classes augmenting the feature vectors), Bp-NoComp (Back-propagation with no augmentation), Rprop-Comp (Resilient Back-propagation with classes augmenting the feature vectors), and Rprop-NoComp (Resilient Back-propagation with no augmentation).

The results for HMC-LMLP and {\it hm}Ant-Miner are the mean and standard deviation over 10 executions. Each HMC-LMLP execution was performed with randomly initialized weights. Clus-HMC, Clus-HSC, and Clus-SC are deterministic algorithms, and thus require a single execution. We highlight the best absolute~values.

\begin{table*}[ht]
\scriptsize
\centering
\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.3}
\caption{$AU(\overline{PRC})$ values obtained}
\begin{tabular*}{\textwidth}
    {@{\extracolsep{\fill}}lcccccccc}
\toprule
Dataset & Bp-Comp  & Bp-NoComp & Rprop-Comp & Rprop-NoComp & Clus-HMC & Clus-HSC & Clus-SC & {\it hm}Ant-Miner \\ 
\midrule
 
Cellcycle & 0.352 $\pm$ 0.0025 & 0.359 $\pm$ 0.0007 & 0.357 $\pm$ 0.001 & 0.365 $\pm$ 0.001 & 0.357 & {\bf 0.371} & 0.252 & 0.325 $\pm$ 0.0079\\

Church  & 0.336 $\pm$ 0.0015 & 0.340 $\pm$ 0.0011 & 0.341 $\pm$ 0.003 & 0.347 $\pm$ 0.001 & 0.348 & {\bf 0.397} & 0.289 & 0.334 $\pm$ 0.0010\\

Derisi    & 0.336 $\pm$ 0.0013 & 0.345 $\pm$ 0.0006 & 0.336 $\pm$ 0.002 & 0.349 $\pm$ 0.001 & {\bf 0.355} & 0.349 & 0.218 & 0.321 $\pm$ 0.0068\\

Eisen     & 0.393 $\pm$ 0.0014 & 0.395 $\pm$ 0.0012 & 0.396 $\pm$ 0.003 & {\bf 0.403 $\pm$ 0.001} & 0.380 & 0.365 & 0.270 & 0.373 $\pm$ 0.0110\\

Gasch1  & 0.373 $\pm$ 0.0029 & 0.378 $\pm$ 0.0012 & 0.372 $\pm$ 0.004 & {\bf 0.384 $\pm$ 0.001} & 0.371 & 0.351 & 0.239 & 0.352 $\pm$ 0.0082\\

Gasch2  & 0.359 $\pm$ 0.0019 & 0.362 $\pm$ 0.0012 & 0.359 $\pm$ 0.003 & 0.369 $\pm$ 0.001 & 0.365 & {\bf 0.378} & 0.267 & 0.334 $\pm$ 0.0165\\

Pheno    & 0.315 $\pm$ 0.0025 & 0.322 $\pm$ 0.0011 & 0.316 $\pm$ 0.002 & 0.325 $\pm$ 0.002 & 0.337 & {\bf 0.416} & 0.316 & 0.336 $\pm$ 0.0017\\

Spo        & 0.334 $\pm$ 0.0021 & 0.340 $\pm$ 0.0007 & 0.332 $\pm$ 0.003 & 0.345 $\pm$ 0.001 & 0.352 & {\bf 0.371} & 0.213 & 0.329 $\pm$ 0.0078\\

Expr       & 0.369 $\pm$ 0.0031 & 0.371 $\pm$ 0.0014 & 0.373 $\pm$ 0.003 & {\bf 0.384 $\pm$ 0.002} & 0.368 & 0.351 & 0.249 & 0.343 $\pm$ 0.0066\\

Seq        & 0.368 $\pm$ 0.0034 & 0.368 $\pm$ 0.0018 & 0.375 $\pm$ 0.003 & 0.384 $\pm$ 0.002 & {\bf 0.386} & 0.282 & 0.197 & 0.371 $\pm$ 0.0069\\

\midrule
Average & 0.355 & 0.358 & 0.356 & {\bf 0.365} & 0.362 & 0.363 & 0.251 & 0.342           \\ 
\bottomrule
\end{tabular*}
\label{tab:prcurves}
\end{table*}

We also compared the HMC methods considering specific classes with the goal of examining their behavior when predicting classes in different hierarchical levels. Since Clus-HMC is considered to be the state-of-the-art method, we performed the comparisons in the Seq dataset, in which Clus-HMC showed the best results according to Table~\ref{tab:prcurves}.

We selected, for each level, the three classes where Clus-HMC obtained its best results. We went down until the seventh hierarchical level. Results are shown in Table~\ref{tab:classesSeq}. The best absolute values are highlighted.

\begin{table*}[ht]
\scriptsize
\centering
\setlength{\tabcolsep}{3pt}
%\renewcommand{\arraystretch}{1.3}
\caption{Best $AU(\overline{PRC})$ obtained in specific classes for the Seq dataset.}
\begin{tabular*}{\textwidth}
    {@{\extracolsep{\fill}}clccccccccc}
\toprule
Level & Classes  & Bp-Comp & Bp-NoComp & Rprop-Comp & Rprop-NoComp & Clus-HMC & Clus-HSC & Clus-SC & {\it hm}Ant-Miner  \\
 \midrule
1 & GO:0044464  & 0.963 $\pm$ 0.006 & {\bf 0.966 $\pm$ 0.002} & 0.965 $\pm$ 0.002 & 0.964 $\pm$ 0.006 & 0.960 & 0.951 & 0.951 & 0.953 $\pm$ 0.0054 \\
1 & GO:0009987  & 0.869 $\pm$ 0.005 & 0.868 $\pm$ 0.004 & 0.870 $\pm$ 0.004 & {\bf 0.873 $\pm$ 0.006} & 0.872 & 0.844 & 0.844 & 0.870 $\pm$ 0.0072 \\
1 & GO:0008152  & 0.791 $\pm$ 0.010 & 0.790 $\pm$ 0.008 & {\bf 0.797 $\pm$ 0.007} & 0.796 $\pm$ 0.006 & 0.774 & 0.700 & 0.700 & 0.730 $\pm$ 0.0098 \\

2 & GO:0044424  & 0.934 $\pm$ 0.002 & {\bf 0.937 $\pm$ 0.003} & 0.936 $\pm$ 0.003 & 0.935 $\pm$ 0.003 & 0.922 & 0.897 & 0.894 & 0.916 $\pm$ 0.0051 \\
2 & GO:0044237  & 0.723 $\pm$ 0.011 & 0.734 $\pm$ 0.009 & 0.734 $\pm$ 0.007 & {\bf 0.742 $\pm$ 0.009} & 0.714 & 0.694 & 0.686 & 0.685 $\pm$ 0.0123 \\
2 & GO:0044238  & 0.690 $\pm$ 0.012 & 0.691 $\pm$ 0.010 & {\bf 0.702 $\pm$ 0.009} & 0.701 $\pm$ 0.006 & 0.664 & 0.632 & 0.634 & 0.651 $\pm$ 0.0142 \\

3 & GO:0044446  & {\bf 0.691 $\pm$ 0.007} & 0.687 $\pm$ 0.007 & 0.677 $\pm$ 0.011 & 0.674 $\pm$ 0.005 & 0.649 & 0.610 & 0.564 & 0.615 $\pm$ 0.0162 \\
3 & GO:0044444  & 0.648 $\pm$ 0.014 & {\bf 0.652 $\pm$ 0.009} & 0.630 $\pm$ 0.014 & 0.636 $\pm$ 0.008 & 0.629 & 0.546 & 0.530 & 0.570 $\pm$ 0.0083 \\
3 & GO:0043229  & 0.591 $\pm$ 0.008 & 0.597 $\pm$ 0.007 & {\bf 0.601 $\pm$ 0.007} & 0.595 $\pm$ 0.006 & 0.584 & 0.554 & 0.547 & 0.561 $\pm$ 0.0109 \\

4 & GO:0043231  & 0.558 $\pm$ 0.009 & 0.562 $\pm$ 0.008 & {\bf 0.567 $\pm$ 0.008} & 0.561 $\pm$ 0.004 & 0.535 & 0.500 & 0.485 & 0.514 $\pm$ 0.0081 \\
4 & GO:0044428  & 0.491 $\pm$ 0.019 & 0.495 $\pm$ 0.015 & 0.496 $\pm$ 0.013 & {\bf 0.497 $\pm$ 0.019} & 0.446 & 0.353 & 0.346 & 0.409 $\pm$ 0.0256 \\
4 & GO:0044267  & 0.428 $\pm$ 0.015 & {\bf 0.460 $\pm$ 0.010} & 0.450 $\pm$ 0.011 & 0.458 $\pm$ 0.012 & 0.383 & 0.337 & 0.329 & 0.376 $\pm$ 0.0240 \\

5 & GO:0006412  & 0.684 $\pm$ 0.014 & {\bf 0.684 $\pm$ 0.012} & 0.664 $\pm$ 0.018 & 0.678 $\pm$ 0.012 & 0.491 & 0.440 & 0.501 & 0.435 $\pm$ 0.0279 \\
5 & GO:0005634  & 0.321 $\pm$ 0.013 & 0.323 $\pm$ 0.017 & 0.317 $\pm$ 0.009 & 0.320 $\pm$ 0.011 & {\bf 0.327} & 0.241 & 0.283 & 0.322 $\pm$ 0.0192 \\
5 & GO:0005739  & 0.395 $\pm$ 0.016 & {\bf 0.403 $\pm$ 0.007} & 0.390 $\pm$ 0.012 & 0.386 $\pm$ 0.014 & 0.308 & 0.284 & 0.310 & 0.244 $\pm$ 0.0096 \\

6 & GO:0045449  & {\bf 0.239 $\pm$ 0.020} & 0.226 $\pm$ 0.016 & 0.191 $\pm$ 0.013 & 0.218 $\pm$ 0.023 & 0.167 & 0.106 & 0.127 & 0.188 $\pm$ 0.0267 \\
6 & GO:0017111  & 0.115 $\pm$ 0.024 & 0.122 $\pm$ 0.028 & 0.301 $\pm$ 0.033 & {\bf 0.304 $\pm$ 0.030} & 0.134 & 0.190 & 0.200 & 0.071 $\pm$ 0.0053 \\
6 & GO:0043687  & 0.227 $\pm$ 0.027 & 0.218 $\pm$ 0.026 & {\bf 0.236 $\pm$ 0.026} & 0.235 $\pm$ 0.026 & 0.105 & 0.096 & 0.120 & 0.115 $\pm$ 0.0136 \\

7 & GO:0006355  & {\bf 0.233 $\pm$ 0.017} & 0.225 $\pm$ 0.016 & 0.183 $\pm$ 0.010 & 0.209 $\pm$ 0.021 & 0.173 & 0.100 & 0.117 & 0.174 $\pm$ 0.0217 \\
7 & GO:0016568  & 0.099 $\pm$ 0.014 & 0.092 $\pm$ 0.012 & 0.093 $\pm$ 0.006 & {\bf 0.121 $\pm$ 0.011} & 0.094 & 0.051 & 0.058 & 0.079 $\pm$ 0.0128 \\
7 & GO:0000723  & 0.081 $\pm$ 0.008 & {\bf 0.086 $\pm$ 0.011} & 0.082 $\pm$ 0.012 & 0.081 $\pm$ 0.009 & 0.085 & 0.064 & 0.056 & 0.082 $\pm$ 0.0104 \\
\midrule
& Average & 0.513 & 0.515 & 0.518 & {\bf 0.523} & 0.477 & 0.438 & 0.442 & 0.455        \\
\bottomrule
\end{tabular*}
\label{tab:classesSeq}
\end{table*}
