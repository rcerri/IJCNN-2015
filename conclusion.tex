\section{Conclusion}\label{sec:conclusion}
In this paper, we presented {\bf H}ierarchical {\bf M}ulti-Label {\bf C}lassification with {\bf L}ocal {\bf M}ulti-{\bf L}ayer {\bf P}erceptron (HMC-LMLP), which associates an MLP to each level of a DAG-structure class hierarchy. HMC-LMLP employs the Local Classifier per Level (LCL)~\cite{Silla2010} strategy, complementing the feature vectors of the instances with their true classes, in order to make use of local information within each level of the hierarchy. With this, we try to avoid problems such as the loss of label dependency during training.

We tested HMC-LMLP on ten datasets related to protein function prediction, in which the protein functions were organized following the Gene Ontology. We compared its performance against the state-of-the-art methods in the literature of HMC, and the empirical analysis indicates that HMC-LMLP matches the state-of-the-art methods in terms of predictive performance, often presenting better results in specific classes of the DAG hierarchy.

We also showed that the use of true labels to complement the feature vectors did not improve the classification performance. We also showed that this may have happened because the adaptation performed in the DAG taxonomies, which resulted in no use of the complete information regarding parent-child class relationship.

As future work, we intend to investigate other neural network training algorithms, such as Extreme Learning Machines~\cite{Huang2004} and Improved Resilient Back-propagation~\cite{Igel2000}. We will also investigate alternatives to adapt the DAG taxonomies to be used with HMC-LMLP, trying to overcome the disadvantages of the currently used adaptation. Other strategies for correct inconsistencies in the prediction will also be tried. Finally, we plan to incorporate other knowledge source in the training process, such as protein-protein interactions networks, and also use HMC-LMLP in other application domains, such as text classification.